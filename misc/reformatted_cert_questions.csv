id,question,answer,original_id
1,"A data engineer is troubleshooting a connection issue with their data warehouse. They need to verify the database connection and check for any potential issues with their dbt project setup. The engineer is encountering error messages indicating a failure to connect to the database, which may be due to incorrect credentials or network issues. Which command provides the most relevant debugging information for verifying the database connection?
================================================
ANSWERS
A) dbt test - Runs tests on models to ensure data quality

B) dbt debug - Tests the database connection and provides setup information

C) dbt parse - Parses the project files for errors

D) dbt run - Executes the models in the project

E) dbt compile - Compiles the models without running them
================================================
TOPICS:
connection
database connections, debugging, dbt commands","================================================
B
================================================
The 'dbt debug' command is specifically designed to test the database connection and provide information about the dbt project setup, including the validity of the project file and the installation of requisite dependencies. Other commands like 'dbt test', 'dbt run', 'dbt compile', and 'dbt parse' serve different purposes and do not focus on debugging the connection.",1
2,"A data engineer is tasked with creating a YAML file for a new analysis in a dbt project. They need to ensure that the analysis includes specific columns with appropriate metadata, such as descriptions and data types, while adhering to the correct YAML structure. What are the essential components that must be included in the YAML file for the analysis to be valid, according to dbt's YAML structure?
================================================
ANSWERS
A) The version number, analysis name, and a list of columns with names, descriptions, and data types, but the data types can be inferred from the data and do not need to be explicitly specified

B) The version number, analysis name, and a list of columns with their names, descriptions, and data types

C) The version number, analysis name, and a list of columns with names and descriptions only, as the data types can be automatically determined by dbt during the analysis process

D) The version number, analysis name, and a list of columns with names only, as the descriptions and data types are not necessary for the analysis to function

E) Only the analysis name and a list of columns without any metadata, as the descriptions and data types can be inferred from the data itself
================================================
TOPICS:
yaml,model","================================================
B
================================================
The essential components for a valid YAML file in a dbt analysis include the version number, the analysis name, and a detailed list of columns that must specify their names, descriptions, and data types. This structure ensures that the analysis is properly defined and can be executed within the dbt framework. The other options lack necessary metadata or components required for a valid configuration.",2
3,"During a dbt project, a data engineer needs to ensure that their transformations are efficient and do not compromise data integrity. They are considering running multiple dbt commands simultaneously to speed up the process. What is the best practice regarding executing dbt commands in parallel to ensure data integrity?
================================================
ANSWERS
A) dbt compile and dbt run

B) dbt test and dbt build

C) dbt build and dbt parse

D) dbt run and dbt test
================================================
TOPICS:
test","================================================
C
================================================
The combination of 'dbt build' and 'dbt parse' can be executed in parallel without risking data integrity, as 'dbt parse' is a read-only command that does not modify data. The other combinations involve at least one write command, which could lead to conflicts if run simultaneously.",3
4,"A data engineer is configuring a dbt project and needs to specify where the snapshots are located. They want to use a custom directory named `archives` instead of the default `snapshots` directory. What should the engineer include in the `dbt_project.yml` file to set the snapshot paths correctly, focusing on the `snapshot-paths` key?
================================================
ANSWERS
A) Set the `snapshot-paths` key in the `dbt_project.yml` file to `archives` to specify the custom directory for snapshots.

B) Leave the `snapshot-paths` key unset to use the default directory for snapshots.

C) Set the `snapshot-paths` key in the `dbt_project.yml` file to `snapshots` to ensure it uses the default directory.

D) Set the `snapshot-paths` key in the `dbt_project.yml` file to `invalid_path` to test the understanding of valid configurations.
================================================
TOPICS:
path,config","================================================
A
================================================
To specify a custom directory for snapshots, the engineer should set the `snapshot-paths` key in the `dbt_project.yml` file to `archives`. This configuration allows dbt to look for snapshots in the specified directory instead of the default `snapshots` directory. Leaving the key unset would result in using the default directory, while setting it back to `snapshots` would negate the purpose of using a custom directory. Creating a new directory within `archives` is unnecessary and does not align with the configuration requirements.",4
5,"A data engineer is tasked with creating a YAML file for a new analysis in a dbt project. They need to ensure that the analysis includes specific columns with appropriate metadata, such as descriptions and data types, while adhering to the correct YAML structure. What are the essential components that must be included in the YAML file for the analysis to be valid, according to dbt's YAML structure?
================================================
ANSWERS
A) The version number, analysis name, and a list of columns with names, descriptions, and data types, but the data types can be inferred from the data and do not need to be explicitly specified

B) The version number, analysis name, and a list of columns with their names, descriptions, and data types

C) The version number, analysis name, and a list of columns with names and descriptions only, as the data types can be automatically determined by dbt during the analysis process

D) The version number, analysis name, and a list of columns with names only, as the descriptions and data types are not necessary for the analysis to function

E) Only the analysis name and a list of columns without any metadata, as the descriptions and data types can be inferred from the data itself
================================================
TOPICS:
yaml,model","================================================
B
================================================
The essential components for a valid YAML file in a dbt analysis include the version number, the analysis name, and a detailed list of columns that must specify their names, descriptions, and data types. This structure ensures that the analysis is properly defined and can be executed within the dbt framework. The other options lack necessary metadata or components required for a valid configuration. ## Reference: https://docs.getdbt.com/reference/resource-properties/columns",5
6,"During a dbt project, a data engineer needs to ensure that their transformations are efficient and do not compromise data integrity. They are considering running multiple dbt commands simultaneously to speed up the process. What is the best practice regarding executing dbt commands in parallel to ensure data integrity?
================================================
ANSWERS
A) dbt compile and dbt run

B) dbt test and dbt build

C) dbt build and dbt parse

D) dbt run and dbt test
================================================
TOPICS:
test","================================================
C
================================================
The combination of 'dbt build' and 'dbt parse' can be executed in parallel without risking data integrity, as 'dbt parse' is a read-only command that does not modify data. The other combinations involve at least one write command, which could lead to conflicts if run simultaneously.",6
7,"A data analyst is responsible for creating a snapshot of the 'orders' table from the 'jaffle_shop' database. The analyst needs to ensure that any changes in the 'status' and 'is_cancelled' columns are tracked over time, allowing for historical analysis. They choose to implement the `check` strategy for this snapshot, which is designed to efficiently monitor specific changes in the data. How should the analyst configure the snapshot to track changes specifically in the 'status' and 'is_cancelled' columns, while maintaining efficiency in data monitoring?
================================================
ANSWERS
A) Set `check_cols='all'` to track changes in every column of the table.

B) Set `check_cols=['status', 'is_cancelled']` to track changes only in the specified columns.

C) Set `check_cols=['id']` to track changes in the primary key only.

D) Set `check_cols=['status', 'is_cancelled', 'total_price']` to track changes in the specified columns plus 'total_price'.
================================================
TOPICS:
snapshot","================================================
B
================================================
The correct configuration ensures that the snapshot specifically tracks changes in the 'status' and 'is_cancelled' columns, making it efficient by focusing only on the relevant data. Tracking all columns or unnecessary ones would create overhead, and tracking the primary key alone would not capture the changes the analyst is interested in.",7
8,"A data engineer is tasked with ensuring the quality of data sourced from Stripe for customer analytics. They need to implement a test to verify that each customer has a unique identifier in the database. The engineer decides to write a test in the dbt model to check the uniqueness of the customer_id column. What is the most effective way for the engineer to ensure uniqueness of customer IDs within the dbt testing framework?
================================================
ANSWERS
A)
Add a test configuration that specifies `unique` for the `customer_id` column in the sources section.

B)
Implement a SQL query that counts the occurrences of each `customer_id` and checks for duplicates.

C)
Create a separate dbt model that filters out non-unique customer IDs before the analytics model runs.

D)
Use a `unique` constraint in the database schema instead of dbt.
================================================
TOPICS:
test","================================================
A
================================================
The correct approach is to add a test configuration that specifies `unique` for the `customer_id` column in the sources section. This ensures that dbt will automatically check for uniqueness during the run. While counting occurrences can help identify duplicates, it does not integrate with dbt's testing framework. Filtering out non-unique IDs in a separate model is not efficient, and using a `unique` constraint in the database schema does not leverage dbt's built-in testing capabilities.",8
9,"A data engineer is tasked with configuring a resource in a YAML file for a data quality test. The test needs to ensure that the data meets certain criteria, including a limit on the number of errors allowed and a specific severity level for reporting issues. The engineer is considering how to set the configuration parameters correctly. What is the correct configuration to ensure that a data quality test allows a maximum of 5 errors and categorizes them as warnings
================================================
ANSWERS
A) Set `limit: 5` and `severity: info` in the config section.

B) Set `limit: 10` and `severity: warn` in the config section.

C) Set `limit: 5` and `severity: warn` in the config section.

D) Set `limit: 5` and `severity: error` in the config section.
================================================
TOPICS:
test,yaml","================================================
C
================================================
The correct configuration requires setting the limit to 5 and the severity to 'warn' to ensure that any issues are reported as warnings. Setting the severity to 'error' would indicate a more critical issue, which is not desired in this case. A limit of 10 would exceed the specified requirement, and using 'info' would not trigger any alerts for the errors.",9
10,"A data engineer is tasked with managing access to various dbt models within a team. They need to ensure that team members can only access the models relevant to their roles while maintaining overall project security. What approach best balances security and usability for access management in dbt?
================================================
ANSWERS
A) Use environment variables to control access to models based on user roles.

B) Assign access based on team members' seniority levels.

C) Create a single role for all team members to simplify access management.

D) Utilize grants to assign specific permissions to team members for each model.

E) Implement a manual approval process for every model access request.
================================================
TOPICS:
security","================================================
D
================================================
Utilizing grants allows the data engineer to assign specific permissions to team members for each model, ensuring that individuals only have access to the models necessary for their roles. This method enhances security and maintains a clear structure for access management. The other options either compromise security, complicate management, or are not practical for effective access control.",10
11,"A data engineer is tasked with optimizing the performance of a data transformation process in a dbt project. They are considering different materialization strategies to improve query speed while managing build times effectively. Which materialization strategy should the data engineer choose to ensure fast query performance while minimizing rebuild times for complex transformations, considering the trade-offs between speed and rebuild times?
================================================
ANSWERS
A) Select ephemeral models to keep the data warehouse clean, but they cannot be queried directly.

B) Opt for incremental models to reduce build times by only transforming new records, but require extra configuration.

C) Use the table materialization for fast querying, but be aware of longer rebuild times for complex transformations.

D) Implement materialized views to combine the benefits of views and tables, allowing for efficient data retrieval and updates.
================================================
TOPICS:
materialization","================================================
C
================================================
The table materialization is the best choice for ensuring fast query performance, especially for models being queried by BI tools. However, the data engineer must be mindful that while tables provide speed, they can take longer to rebuild, particularly for complex transformations. Incremental models are beneficial for reducing build times but require additional configuration and are best suited for event-style data. Ephemeral models, while useful for lightweight transformations, cannot be queried directly, and materialized views serve a similar purpose to incremental models but may not be the best fit for all scenarios.",11
12,"A data engineer is configuring a dbt project and needs to specify where the snapshots are located. They want to use a custom directory named `archives` instead of the default `snapshots` directory. What should the engineer include in the `dbt_project.yml` file to set the snapshot paths correctly, focusing on the `snapshot-paths` key?
================================================
ANSWERS
A) Set the `snapshot-paths` key in the `dbt_project.yml` file to `archives` to specify the custom directory for snapshots.

B) Leave the `snapshot-paths` key unset to use the default directory for snapshots.

C) Set the `snapshot-paths` key in the `dbt_project.yml` file to `snapshots` to ensure it uses the default directory.

D) Set the `snapshot-paths` key in the `dbt_project.yml` file to `invalid_path` to test the understanding of valid configurations.
================================================
TOPICS:
path,config","================================================
A
================================================
To specify a custom directory for snapshots, the engineer should set the `snapshot-paths` key in the `dbt_project.yml` file to `archives`. This configuration allows dbt to look for snapshots in the specified directory instead of the default `snapshots` directory. Leaving the key unset would result in using the default directory, while setting it back to `snapshots` would negate the purpose of using a custom directory. Creating a new directory within `archives` is unnecessary and does not align with the configuration requirements.",12
13,"A data analyst is working on a dbt project and needs to consolidate multiple SQL transformations that depend on the same raw tables. They want to ensure that their modeling work is efficient and easy to trace. What approach should the analyst take to achieve efficient dependency tracing in their dbt project, particularly in terms of maintainability and ease of understanding?
================================================
ANSWERS
A) Create a single model that combines all transformations

B) Avoid using sources and directly reference tables in models

C) Use sources to define dependencies on raw tables

D) Implement a complex stored procedure for data transformations
================================================
TOPICS:
modelling","================================================
C
================================================
Using sources in dbt allows the analyst to define dependencies on raw tables clearly, making it easier to trace which models depend on which sources. Creating a single model that combines all transformations can lead to complexity and hinder maintainability, while avoiding sources may hinder maintainability. Implementing stored procedures is not aligned with dbt's intended use.",13
14,"A data analyst is tasked with configuring a dbt project to ensure that a model representing total sales is user-friendly in the database. They want to alias the model from its default name to something more descriptive for end-users. What should the analyst include in the `dbt_project.yml` file to alias the model `sales_total` as `sales_dashboard`?
================================================
ANSWERS
A) models:
  your_project:
    sales_total:
      +name: sales_dashboard

B) models:
  your_project:
    sales_total:
      alias: sales_dashboard

C) models:
  your_project:
    sales_total:
      +alias: sales_dashboard

D) models:
  your_project:
    sales_total:
      alias: sales_total_v2
================================================
TOPICS:
modelling","================================================
C
================================================
To correctly alias the model `sales_total` as `sales_dashboard`, the analyst should use the syntax that includes the `+alias` directive. This ensures that the model is recognized under the new alias in the database. The other options either use incorrect syntax or do not include the necessary `+` symbol, which is required for dbt configurations.",14
15,"A data analyst is responsible for creating a snapshot of the 'orders' table from the 'jaffle_shop' database. The analyst needs to ensure that any changes in the 'status' and 'is_cancelled' columns are tracked over time, allowing for historical analysis. They choose to implement the `check` strategy for this snapshot, which is designed to efficiently monitor specific changes in the data. How should the analyst configure the snapshot to track changes specifically in the 'status' and 'is_cancelled' columns, while maintaining efficiency in data monitoring?
================================================
ANSWERS
A) Set `check_cols='all'` to track changes in every column of the table.

B) Set `check_cols=['status', 'is_cancelled']` to track changes only in the specified columns.

C) Set `check_cols=['id']` to track changes in the primary key only.

D) Set `check_cols=['status', 'is_cancelled', 'total_price']` to track changes in the specified columns plus 'total_price'.
================================================
TOPICS:
snapshot","================================================
B
================================================
The correct configuration ensures that the snapshot specifically tracks changes in the 'status' and 'is_cancelled' columns, making it efficient by focusing only on the relevant data. Tracking all columns or unnecessary ones would create overhead, and tracking the primary key alone would not capture the changes the analyst is interested in.",15
16,"Question 33 of 67
A data engineer is tasked with ensuring the quality of data sourced from Stripe for customer analytics. They need to implement a test to verify that each customer has a unique identifier in the database. The engineer decides to write a test in the dbt model to check the uniqueness of the customer_id column. What is the most effective way for the engineer to ensure uniqueness of customer IDs within the dbt testing framework?
================================================
ANSWERS
A) Add a test configuration that specifies `unique` for the `customer_id` column in the sources section.

B) Implement a SQL query that counts the occurrences of each `customer_id` and checks for duplicates.

C) Create a separate dbt model that filters out non-unique customer IDs before the analytics model runs.

D) Use a `unique` constraint in the database schema instead of dbt.
================================================
TOPICS:
test","================================================
A
================================================
The correct approach is to add a test configuration that specifies `unique` for the `customer_id` column in the sources section. This ensures that dbt will automatically check for uniqueness during the run. While counting occurrences can help identify duplicates, it does not integrate with dbt's testing framework. Filtering out non-unique IDs in a separate model is not efficient, and using a `unique` constraint in the database schema does not leverage dbt's built-in testing capabilities.",16
17,"A data engineer is tasked with configuring a resource in a YAML file for a data quality test. The test needs to ensure that the data meets certain criteria, including a limit on the number of errors allowed and a specific severity level for reporting issues. The engineer is considering how to set the configuration parameters correctly. What is the correct configuration to ensure that a data quality test allows a maximum of 5 errors and categorizes them as warnings
================================================
ANSWERS
A) Set `limit: 5` and `severity: info` in the config section.

B) Set `limit: 10` and `severity: warn` in the config section.

C) Set `limit: 5` and `severity: warn` in the config section.

D) Set `limit: 5` and `severity: error` in the config section.
================================================
TOPICS:
test,yaml","================================================
C
================================================
The correct configuration requires setting the limit to 5 and the severity to 'warn' to ensure that any issues are reported as warnings. Setting the severity to 'error' would indicate a more critical issue, which is not desired in this case. A limit of 10 would exceed the specified requirement, and using 'info' would not trigger any alerts for the errors.",17
18,"A data engineer is tasked with managing access to various dbt models within a team. They need to ensure that team members can only access the models relevant to their roles while maintaining overall project security. What approach best balances security and usability for access management in dbt?
================================================
ANSWERS
A) Use environment variables to control access to models based on user roles.

B) Assign access based on team members' seniority levels.

C) Create a single role for all team members to simplify access management.

D) Utilize grants to assign specific permissions to team members for each model.

E) Implement a manual approval process for every model access request.
================================================
TOPICS:
security","================================================
D
================================================
Utilizing grants allows the data engineer to assign specific permissions to team members for each model, ensuring that individuals only have access to the models necessary for their roles. This method enhances security and maintains a clear structure for access management. The other options either compromise security, complicate management, or are not practical for effective access control.",18
19,"A data engineer is tasked with optimizing the performance of a data transformation process in a dbt project. They are considering different materialization strategies to improve query speed while managing build times effectively. Which materialization strategy should the data engineer choose to ensure fast query performance while minimizing rebuild times for complex transformations, considering the trade-offs between speed and rebuild times?
================================================
ANSWERS
A) Select ephemeral models to keep the data warehouse clean, but they cannot be queried directly.

B) Opt for incremental models to reduce build times by only transforming new records, but require extra configuration.

C) Use the table materialization for fast querying, but be aware of longer rebuild times for complex transformations.

D) Implement materialized views to combine the benefits of views and tables, allowing for efficient data retrieval and updates.
================================================
TOPICS:
materialization","================================================
C
================================================
The table materialization is the best choice for ensuring fast query performance, especially for models being queried by BI tools. However, the data engineer must be mindful that while tables provide speed, they can take longer to rebuild, particularly for complex transformations. Incremental models are beneficial for reducing build times but require additional configuration and are best suited for event-style data. Ephemeral models, while useful for lightweight transformations, cannot be queried directly, and materialized views serve a similar purpose to incremental models but may not be the best fit for all scenarios.",19
20,"A data analyst is working on a dbt project that involves multiple models and needs to ensure that their database connection is functioning properly before continuing with development. They also want to verify the validity of their project configuration and check for any missing dependencies. Which command should the analyst use to test both the database connection and the project configuration, including any missing dependencies?
================================================
ANSWERS
A) `dbt debug` to test the connection, validate the project file, and check for missing dependencies.

B) `dbt debug --connection` to only test the database connection without additional configuration checks.

C) `dbt debug --config-dir` to locate the `profiles.yml` file and test the configuration settings.

D) `dbt run --debug` to execute the models with detailed output but not specifically test the connection.
================================================
TOPICS:
dbt commands, database connections, project configuration, dependency management, dbt debug","================================================
A
================================================
The correct command is `dbt debug`, which not only tests the database connection but also checks for any issues in the project configuration and identifies missing dependencies. The command `dbt debug --connection` only tests the connection and skips additional checks, while `dbt debug --config-dir` only locates the `profiles.yml` file. The command `dbt run --debug` is used for running models with increased verbosity but does not serve as a connection test.",20
21,"In a dbt project, what happens if a model fails during a CI build?
================================================
ANSWERS
A) The model is automatically fixed by dbt

B) The CI build is considered successful

C) The CI build stops and the failure is reported

D) The model is excluded from future CI builds
================================================
TOPICS:
ci/cd, error handling, testing","================================================
C
================================================
The CI build stops and the failure is reported, preventing the faulty code from being merged into the main branch.",21
22,"What are ""macros"" in the context of dbt projects?
================================================
ANSWERS
A) Code blocks for configuring models

B) Blocks of code for bulk configurations

C) Blocks of SQL queries

D) Docs for your project
================================================
TOPICS:
macros, sql, code reusability, jinja templating","================================================
C
================================================
Macros are blocks of code that you can reuse multiple times.
Reference:https://docs.getdbt.com/docs/build/projects",22
23,"What is the recommended approach for snapshotting source data in its raw form?
================================================
ANSWERS
A) Include all columns

B) Apply business logic

C) Use joins in the snapshot

D) Select specific columns
================================================
TOPICS:
snapshots, source data, raw data, data modeling, incremental models","================================================
A
================================================
Include all columns. Snapshot source data in its raw form and include as many columns as possible, even if not immediately needed.",23
24,"Why should you use consistent file naming patterns in dbt staging models?
================================================
ANSWERS
A) To make the code look prettier

B) To impress stakeholders

C) To avoid naming conflicts

D) To confuse data engineers
================================================
TOPICS:
naming conventions, code organization, maintainability","================================================
C
================================================
To avoid naming conflicts. Consistent file naming patterns help avoid naming conflicts and improve clarity.",24
25,"What is the key advantage of materialized view materializations in dbt compared to regular views?
================================================
ANSWERS
A) Data freshness

B) Query performance

C) Simplicity of use

D) Manual refresh
================================================
TOPICS:
materializations, query performance, data freshness, views, tables","================================================
B
================================================
Materialized view materializations offer the advantage of combining the query performance of a table with the data freshness of a view. Reference:https://docs.getdbt.com/docs/build/materializations",25
26,"What is the primary purpose of using sources in dbt?
================================================
ANSWERS
A) To optimize query performance

B) To document and test data transformations

C) To name and describe data loaded into your warehouse

D) To automate data loading processes
================================================
TOPICS:
sources, data lineage, testing","================================================
C
================================================
To name and describe data loaded into your warehouse. Sources in dbt are used to name and describe the data loaded into your warehouse, enhancing data lineage and testing.",26
27,"What is the recommended approach for code reuse in dbt Python models?
================================================
ANSWERS
A) Create and register ""named"" UDFs

B) Use private Python packages

C) Define functions in SQL models

D) Write code directly in SQL models
================================================
TOPICS:
python models, user-defined functions (udfs), code reusability, dbt best practices","================================================
A
================================================
Create and register ""named"" UDFs. Currently, Python functions defined in one dbt model can't be imported and reused in other models. Reference:https://docs.getdbt.com/docs/build/python-models",27
28,"Where should you store general-purpose models generated from macros or seeds?
================================================
ANSWERS
A) In the staging folder

B) In the utilities folder

C) In the models folder

D) In the base folder
================================================
TOPICS:
dbt project structure, dbt models, dbt macros","================================================
B
================================================
General-purpose models should be stored in the models/utilities directory. Reference:https://docs.getdbt.com/best-practices/how-we-structure/2-staging",28
29,"What should you do if you want to share a dbt project with others?
================================================
ANSWERS
A) Use the Project subdirectory option

B) Initialize a new project

C) Set up a connection with data platform

D) Clone dbt Lab's Jaffle shop on GitHub
================================================
TOPICS:
git, version control, project management","================================================
A
================================================
Use the Project subdirectory option. You can create new projects and share them with other people by making them available on a hosted git repository like GitHub. Reference:https://docs.getdbt.com/docs/build/projects",29
30,"What can be viewed once a dbt job has finished running?
================================================
ANSWERS
A) Only the run status

B) Only the data models

C) Run results and artifacts

D) Only the errors
================================================
TOPICS:
dbt artifacts, job run status, run results","================================================
C
================================================
Run results and artifacts. Post completion, one can view run results and artifacts from that run. Reference:https://courses.getdbt.com/courses/take/advanced-deployment/lessons/39681223-environments-jobs-and-runs",30
31,"Which resource in a dbt project allows you to name and describe the data loaded into your warehouse?
================================================
ANSWERS
A) snapshots

B) seeds

C) sources

D) metrics
================================================
TOPICS:
sources, dbt project structure, data modeling, metadata management","================================================
C
================================================
The ""sources"" resource in a dbt project allows you to name and describe the data loaded into your warehouse.",31
32,"What should you do to improve the readability of Jinja code?
================================================
ANSWERS
A) Use meaningful variable names

B) Use short and cryptic variable names

C) Avoid using variables

D) Use random variable names
================================================
TOPICS:
jinja, code style, readability, best practices, variable naming","================================================
A
================================================
Use meaningful variable names
Examples of Jinja style: {{ this }} instead of {{this}}.
Reference: https://docs.getdbt.com/best-practices/how-we-style/4-how-we-style-our-jinja",32
33,"What is the primary role of semantic models in MetricFlow?
================================================
ANSWERS
A) Basis for defining data

B) Organizing YAML files

C) Writing SQL queries

D) Querying metrics
================================================
TOPICS:
semantic models, metrics, data modeling","================================================
A
================================================
Basis for defining data

Semantic models serve as the basis for defining data in MetricFlow.

Reference: https://docs.getdbt.com/docs/build/build-metrics-intro",33
34,"Can environment variables in dbt Cloud contain secrets?
================================================
ANSWERS
A) No, they are always public

B) Yes, but they are visible in the logs

C) Yes, and they can be configured to be hidden

D) They cannot include secrets due to security policies
================================================
TOPICS:
environment variables, secrets management, dbt cloud, security, logs","================================================
C
================================================
Yes, and they can be configured to be hidden
Environment variables in dbt Cloud can include secrets and can be configured to be hidden from logs and the UI.
Reference: https://courses.getdbt.com/courses/take/advanced-deployment/texts/39437558-review",34
35,"What is the purpose of dbt groups in a DAG?
================================================
ANSWERS
A) To restrict access to private models

B) To organize models in folders

C) To define model dependencies

D) To schedule model runs
================================================
TOPICS:
access control, model organization, collaboration","================================================
A
================================================
To restrict access to private models
A group is a collection of nodes within a dbt DAG. Groups are named, and every group has an owner. They enable intentional collaboration within and across teams by restricting access to private models.
Reference: https://docs.getdbt.com/docs/build/groups",35
36,"In which environments can you develop with MetricFlow in dbt?
================================================
ANSWERS
A) dbt Cloud CLI

B) dbt Cloud IDE

C) dbt Core

D) All of the above
================================================
TOPICS:
dbt cloud, dbt ide, dbt core","================================================
D
================================================
MetricFlow allows you to develop from your preferred environment, whether that's the dbt Cloud CLI, dbt Cloud IDE, or dbt Core.",36
37,"What is a dbt package?
================================================
ANSWERS
A) A storage container

B) A cloud service

C) A library of dbt code

D) A user interface element
================================================
TOPICS:
dbt packages, dbt code reusability, dbt project structure","================================================
C
================================================
Packages are libraries that can be used in dbt projects. Reference: https://docs.getdbt.com/terms/dry",37
38,"In dbt, what does the profiles.yml file manage?
================================================
ANSWERS
A) Model configurations

B) Database connections

C) Project dependencies

D) Data sources
================================================
TOPICS:
database connections, project configuration, environment management","================================================
B
================================================
The profiles.yml file in dbt is used for managing database connections.Reference: https://docs.getdbt.com/dbt-cli/configure-your-profile",38
39,"What type of files are typically stored in the ""docs"" directory in a dbt project?
================================================
ANSWERS
A) Model definitions

B) Project configuration files

C) Documentation for the project

D) Database schema descriptions
================================================
TOPICS:
documentation, dbt project structure, dbt build process","================================================
C
================================================
The ""docs"" directory in a dbt project typically stores documentation for the project.

Reference: https://docs.getdbt.com/docs/build/projects",39
40,"When might you consider using materialized views instead of incremental models in dbt?
================================================
ANSWERS
A) For any use case

B) For simple models

C) When dealing with BigQuery

D) When you need to manage incremental logic
================================================
TOPICS:
materialized views, incremental models, bigquery, dbt materializations","================================================
C
================================================
Materialized views are a suitable choice when incremental models are sufficient, but you want the data platform to manage the incremental logic and refresh.",40
41,"What is the purpose of the ""depends_on"" property in an exposure definition?
================================================
ANSWERS
A) Specify dependencies for the exposure

B) Determine data source dependencies

C) Define exposure maturity

D) List all SQL queries
================================================
TOPICS:
depends_on, exposures, ref, source, metric","================================================
A
================================================
Specify dependencies for the exposure

Expected: depends_on: list of refable nodes, including ref, source, and metric (While possible, it is highly unlikely you will ever need an exposure to depend on a source directly)

Reference: https://docs.getdbt.com/docs/build/exposures",41
42,"Is Slim CI a default feature in dbt, or does it require additional configuration?
================================================
ANSWERS
A) It's a default feature in all dbt projects

B) It requires additional configuration

C) It's only available in dbt Cloud

D) It's not a feature of dbt but of external CI tools
================================================
TOPICS:
slim ci, ci/cd, dbt configuration, dbt cloud","================================================
B
================================================
Slim CI is not a default feature and requires additional configuration to smartly run CI builds for only updated models and their downstream dependencies.

Reference: https://courses.getdbt.com/courses/take/advanced-deployment/texts/39437556-review",42
43,"How can you reference an exposure when running dbt commands?
================================================
ANSWERS
A) -s +exposure:<exposure_name>

B) -e <exposure_name>

C) -r <exposure_name>

D) -x <exposure_name>
================================================
TOPICS:
cli, exposures, selection-syntax, dbt run, dbt commands","================================================
A
================================================
-s +exposure:<exposure_name>
Once an exposure is defined, you can run commands that reference it: dbt run -s +exposure:weekly_jaffle_report
Reference: https://docs.getdbt.com/docs/build/exposures",43
44,"Which tool can be used to generate the correct cron syntax when setting up a custom cron schedule for a deploy job?
================================================
ANSWERS
A) crontab.guru

B) dbt Scheduler

C) cronjobgenerator.com

D) cronmaster.com
================================================
TOPICS:
cron, job scheduling, dbt deployments, dbt cloud","================================================
A
================================================
crontab.guru

Use tools such as crontab.guru to generate the correct cron syntax. This tool allows you to input cron snippets and returns their plain English translations.

Reference: https://docs.getdbt.com/docs/deploy/deploy-jobs",44
45,"What is the function of the dispatch configuration in dbt_project.yml?
================================================
ANSWERS
A) To schedule dbt runs

B) To specify macro namespaces and their search order

C) To dispatch reports

D) To deploy models to different databases
================================================
TOPICS:
macros, namespaces, dispatch configuration, dbt_project.yml, search order","================================================
B
================================================
To specify macro namespaces and their search order

The dispatch configuration is used to specify macro namespaces and their search order.

Reference: https://docs.getdbt.com/reference/dbt_project.yml",45
46,"How does dbt handle model materialization?
================================================
ANSWERS
A) Only in dbt project yaml file

B) Only in model sql file

C) In dbt project yaml file and model sql file

D) Does not handle at all
================================================
TOPICS:
materializations, configuration, model properties, dbt project structure, sql files","================================================
C
================================================
Dbt project materialization can be changed in dbt project yaml file and model sql file. Reference: https://docs.getdbt.com/docs/build/materializations",46
47,"Can dbt be used with multiple data warehouses?
================================================
ANSWERS
A) Yes, but only sequentially

B) No, it is limited to one at a time

C) Yes, simultaneously with multiple configurations

D) It does not support data warehouses
================================================
TOPICS:
dbt configuration, dbt adapters, data warehouses","================================================
C
================================================
dbt can be used with multiple data warehouses simultaneously, each with its configuration. Reference: https://docs.getdbt.com/docs/available-adapters",47
48,"Where should you start debugging an ""Invalid ref function"" error in dbt?
================================================
ANSWERS
A) In the dbt_project.yml file

B) In the profiles.yml file

C) In the referenced SQL file

D) In the dbt logs
================================================
TOPICS:
sql syntax, ref function, debugging, dbt errors","================================================
C
================================================
Start by opening the SQL file where the error occurs. Reference: https://docs.getdbt.com/guides/debug-errors?",48
49,"What does the generate_alias_name macro accept as arguments?
================================================
ANSWERS
A) Node and node version

B) Model name and alias name

C) Model config and node name

D) Custom alias and node name
================================================
TOPICS:
macros, custom aliases, model configuration, nodes","================================================
D
================================================
The generate_alias_name macro accepts the custom alias supplied in the model config and the node for which it's generated.

Reference: https://docs.getdbt.com/docs/build/custom-aliases",49
50,"What can you add to sources in DBT for documentation purposes?
================================================
ANSWERS
A) Descriptions

B) Filters

C) Constraints

D) Data tests
================================================
TOPICS:
sources, documentation, metadata","================================================
A
================================================
You can also add descriptions to sources, that get rendered as part of your documentation site.
Reference: https://docs.getdbt.com/docs/build/sources",50
51,"How can you remove deleted models from your data warehouse using dbt?
================================================
ANSWERS
A) Use 'dbt remove' command

B) Delete the model files

C) Use 'dbt drop' command

D) Use 'dbt refresh' command
================================================
TOPICS:
dbt commands, model management, data warehouse maintenance","================================================
A
================================================
Use 'dbt remove' command

You can remove deleted models from your data warehouse using the 'dbt remove' command.

Reference: https://docs.getdbt.com/docs/build/sql-models",51
52,"What happens when the same unique key is present in both old and new model data in an incremental model?
================================================
ANSWERS
A) The old row is updated with the new row

B) The old row is deleted

C) The new row is appended

D) An error occurs
================================================
TOPICS:
incremental models, unique keys, data updates","================================================
A
================================================
The old row is updated with the new row

When the same unique key is present in both old and new model data, dbt updates/replace the old row with the new row of data.

Reference: https://docs.getdbt.com/docs/build/incremental-models",52
53,"What are some common use cases of intermediate models?
================================================
ANSWERS
A) Materialization as tables

B) Isolating complex operations

C) Exposing to end users

D) Focusing on single entities
================================================
TOPICS:
intermediate models, complex operations, troubleshooting, code organization, single source of truth","================================================
B
================================================
Intermediate models are commonly used for isolating complex operations and making them easier to refine and troubleshoot.",53
54,"What is the primary purpose of reading error messages in dbt?
================================================
ANSWERS
A) To find the line number of the error

B) To identify the type of error

C) To determine the user who caused it

D) To see the error in the logs
================================================
TOPICS:
error handling, debugging, dbt syntax","================================================
B
================================================
The error message dbt produces contains the type of error and the file where the error occurred.",54
55,"How does the scheduler handle CI (continuous integration) jobs?
================================================
ANSWERS
A) Runs CI jobs in parallel

B) Runs CI jobs in serial

C) Gives CI jobs higher priority

D) Requires additional slots
================================================
TOPICS:
job scheduler, ci/cd, concurrency, job prioritization","================================================
A
================================================
Runs CI jobs in parallel

CI jobs are handled differently and execute concurrently in parallel to increase productivity and avoid blocking production runs.

Reference: https://docs.getdbt.com/docs/deploy/job-scheduler",55
56,"What is a con of using materialized views in dbt?
================================================
ANSWERS
A) Limited configuration options

B) Slow query performance

C) No support for incremental updates

D) Incompatible with custom schemas
================================================
TOPICS:
materialized views, configuration, incremental models, dbt materializations","================================================
A
================================================
Limited configuration options
A con of using materialized views is that they tend to have fewer configuration options available.
Reference: https://docs.getdbt.com/docs/build/materializations",56
